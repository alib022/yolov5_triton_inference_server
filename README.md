# yolov5_triton_inference_server
<!-- cd create_engine/
docker run --gpus all -it --rm -v $PWD:/create_engine nvcr.io/nvidia/tensorrt:21.03-py3
mkdir build
cd build/
cmake ..
make -->

